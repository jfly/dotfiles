# misc thrashing about

This looks useful:

    kubectl cluster-info

Namespaces are a thing:

    kubectl get namespaces

`describe` sounds like something worth remembering in general?

    kubectl describe namespaces default

And `get`:

    kubectl get namespaces default

namespaces = namespace = ns

They then explain some imperative CRUD commands that I really don't want to
ever run by hand. YAML please?


> Organizations with high-maturity models restrict access to the dashboard and
> all cluster modifications are applied through the GitOps techniques with
> infrastructure-as-code (IaC). Despite these techniques, having access to the
> dashboard, especially in development, can be mighty handy.

Ok yes IaC please!

Lol talking shit on Tesla!

Urg, not super interested in the dashboard right now. Oh great, it doesn't even work
Getting errors like this:

> for me? namespaces is forbidden: User "system:anonymous" cannot list resource "namespaces" in API group "" at the cluster scope

# k8s fundamentals

Ok, it took me a while to find a good sequence of tutorials, but this looks promising!

I'm going to try working through the "Kubernentes Fundamentals" tutorial starting with https://learning.oreilly.com/scenarios/kubernetes-fundamentals-minikube/9781492078814/.

## 2022-02-06 19:35 -0800: minikube

    $ nix-shell -i minikube kubectl
    [nix-shell:~]$ minikube version
    minikube version: v1.23.2
    commit: v1.23.2
    $ minikube delete --all --purge
    $ minikube start --driver=docker

### steps 1, 2

I forgot to take notes here :p

### 2022-02-06 20:22 -0800: step 3: kubectl

    kubectl [verb] [noun] [identifier]?

verbs:
- get
- describe

nouns:
- nodes
- events


### 2022-02-06 20:27 -0800: step 4: metrics-server. do i really care about this right now?

This doesn't work:

    $ kubectl top node

This is supposed to make that work:

    $ minikube addons enable metrics-server

But it doesn't?

    $ kubectl top node
    Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)

... oh, it just took a while:

    $ kubectl top node
    NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    minikube   83m          2%     659Mi           4%

oohh, i was supposed to watch that deployment with

    $ kubectl get deployments,pods --namespace kube-system --selector k8s-app=metrics-server

too late now, i guess

> While the addons feature is convenient for standing up a small set of helpful
> solutions, it's very minikube centric. All of these solutions and hundreds
> more are better configured and installed using the Helm package manager for
> Kubernetes.

lol, ok i'm going to forget about this, then

    minikube addons enable dashboard

and we're done. on to the next section


### 2022-02-06 20:36 -0800: step 5: services

Look at all these services!

    $ minikube service list
    |----------------------|---------------------------|--------------|-----|
    |      NAMESPACE       |           NAME            | TARGET PORT  | URL |
    |----------------------|---------------------------|--------------|-----|
    | default              | kubernetes                | No node port |
    | kube-system          | kube-dns                  | No node port |
    | kube-system          | metrics-server            | No node port |
    | kubernetes-dashboard | dashboard-metrics-scraper | No node port |
    | kubernetes-dashboard | kubernetes-dashboard      | No node port |
    |----------------------|---------------------------|--------------|-----|


Some talk of services of type `ClusterIP`. Hopefully I'll learn more about that later.

Q: Why does `kubectl get services --all-namespaces` output something different than `kubectl get services`? Is the `default` namespace special?

And we're done.

### 2022-02-06 20:43 -0800: step 6: Dashboard

    $ kubectl get service -n kubernetes-dashboard kubernetes-dashboard
    NAME                   TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
    kubernetes-dashboard   ClusterIP   10.109.45.1   <none>        80/TCP    15m

Still not super clear on why `-n` isn't required.

`kubectl get service --help` has this to say:

> Namespace in current context is ignored even if specified with --namespace.

Not clear on what the "current context" refers to...

Ooh thank you Google! `kubectl config get-contexts`

    $ kubectl patch service -n kubernetes-dashboard kubernetes-dashboard --type='json' -p '[{"op":"replace","path":"/spec/type","value":"NodePort"}]'
    $ kubectl patch service -n kubernetes-dashboard kubernetes-dashboard --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":30000}]'

I still don't care about all this imperative stuff. I thought I could manage all this with YAML files?

    $ minikube service list
    |----------------------|---------------------------|--------------|---------------------------|
    |      NAMESPACE       |           NAME            | TARGET PORT  |            URL            |
    |----------------------|---------------------------|--------------|---------------------------|
    ...
    | kubernetes-dashboard | kubernetes-dashboard      |           80 | http://192.168.49.2:30000 |
    |----------------------|---------------------------|--------------|---------------------------|

Ok, we've got a URL now. It works in my web browser! Oh, or just `minikube dashboard`. Cute.

Pretty neat that the dashboard itself is just another thing on the k8s cluster.
Urg, I really don't want to learn my way around this thing right now, though.

### 2022-02-06 20:57 -0800: step 7: Making Changes

Pretty straightforward. Not much to say here.

    $ kubectl create namespace sandbox-a
    namespace/sandbox-a created
    $ kubectl create namespace sandbox-b
    namespace/sandbox-b created
    $ kubectl get namespaces
    NAME                   STATUS   AGE
    default                Active   43m
    kube-node-lease        Active   43m
    kube-public            Active   43m
    kube-system            Active   43m
    kubernetes-dashboard   Active   27m
    sandbox-a              Active   7s
    sandbox-b              Active   5s
    $ kubectl label namespace sandbox-a group=development
    namespace/sandbox-a labeled
    $ kubectl label namespace sandbox-b customer=alpha
    namespace/sandbox-b labeled
    $ kubectl describe namespace sandbox-a
    Name:         sandbox-a
    Labels:       group=development
                  kubernetes.io/metadata.name=sandbox-a
    Annotations:  <none>
    Status:       Active

    No resource quota.

    No LimitRange resource.

Pretty slick how the webui updated automagically when I made these changes.

And we're done. What's next?

## 2022-02-06 21:01 -0800: First Kubernetes Application

Some vocab here:

- `Services`
- `Pods`
- `resilience`?
- `rollout`?


### 2022-02-06 21:09 -0800: step 1: Your Kubernetes Cluster

Hmm... `helm`? Oh boy, I was really hoping to hold off on learning about *that* for a while :cry:

Let's crank it up to this:

    $ nix-shell -p minikube kubectl kubernetes-helm

    $ helm version --short
    v3.7.1+g1d11fcb

### 2022-02-06 21:14 -0800: step 2: Deployment

Lol, had to copy that file out of the katacoda session. Glad that's back up, but now I'm used to running things locally.

    $ cat echoserver.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        run: hello
      name: hello
    spec:
      replicas: 1
      selector:
        matchLabels:
          run: hello
      template:
        metadata:
          labels:
            run: hello
        spec:
          containers:
          - image: k8s.gcr.io/echoserver:1.9
            name: hello
            ports:
            - containerPort: 8080

lol `ccat` # EDIT: i originally thought that was a typo, but it keeps showing up. it's a "colorized cat" command with syntax highlighting. Neat!

The docs mention `ReplicaSet`, but I don't see that anywhere in the yaml file...

    $ kubectl create -f echoserver.yaml
    $ kubectl get deployments,pods

Ooh, this is the first time we've seen a comma separate list passed to `get`. Neat!
So now it's running, but the section closes out by saying we need to create a
"service" to actually access the pod.


### 2022-02-06 21:23 -0800: step 3 + 4: Service + Scaling

Ok, so services are like a load balancer.

Urg are they *really* going to make me create one of these imperatively? I
*just* got a taste of some yaml in the previous section.

    $ kubectl expose deployment hello --type=NodePort
    service/hello exposed

urg patch again

    $ kubectl patch service hello --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":31001}]'

Ok, this shows up in `minikube service list` now! And http://192.168.49.2:31001/ works.

    $ while true; do curl -s http://192.168.49.2:31001/ -w 'Time: %{time_total}' | grep -E 'Hostname|Time' | xargs; done

In another terminal:

    $ kubectl scale deployment hello --replicas=3

    $ kubectl get pods -l run=hello
    NAME                    READY   STATUS    RESTARTS   AGE
    hello-b7b9bf798-6pnvg   1/1     Running   0          22s
    hello-b7b9bf798-j87cr   1/1     Running   0          16m
    hello-b7b9bf798-t6dzv   1/1     Running   0          22s

    $ k describe service hello | grep Endpoints
    Endpoints:                172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080

Fun!

    $ kubectl scale deployment hello --replicas=0
    $ kubectl scale deployment hello --replicas=1

And that's the end of the section.

### 2022-02-06 21:40 -0800: step 5: Resiliance

Scale back up:

    $ k scale deployment hello --replicas=3
    deployment.apps/hello scaled

Q: What's the difference between `k get pods --selector=run=hello` and `k get pods -l run=hello`?

Here's a command I trust :p

    $ kubectl delete --now pod $(kubectl get pods --selector=run=hello | sed '2!d' | cut -d' ' -f1) > /dev/null &
    $ watch kubectl get pods --selector=run=hello

Ok, that happened too fast for me to notice. :shrug:

### 2022-02-06 21:46 -0800: step 6: Rollout

Urg I do not care to learn about this "precise surgical way" with `kubectl set image`. I don't see myself ever wanting to do that?

    # first, edit echoserver.yaml
    $ kubectl replace -f echoserver.yaml

And we're done. No `helm`? Phew!

## 2022-02-06 21:51 -0800: Pods to Services Communication

Interesting motivation with the "Twelve-Factor App Methodology". Not what I would have gone for, but hey.

Let's dive in.

### 2022-02-06 21:54 -0800: step 1: Your Kubernetes Cluster

lol, ok. just some setup stuff. done!

### 2022-02-06 21:54 -0800: step 2: Communication from Pods to Services

`services` have many `pod`s (aka containers?)

> In fact, within your container code there should typically be no coupling to the Kubernetes ecosystem.

I like this!

    $ kubectl apply -f https://raw.githubusercontent.com/javajon/kubernetes-fundamentals/master/nginx/nginx.yaml

    $ curl https://raw.githubusercontent.com/javajon/kubernetes-fundamentals/master/nginx/nginx.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: nginx-pod-port
        protocol: TCP
        name: web
      selector:
        app: nginx
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment
    spec:
      selector:
        matchLabels:
          app: nginx
      replicas: 2
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.15.1
            ports:
            - containerPort: 80
              name: nginx-pod-port

Oooh, I like this YAML file! It declares a Service and a Deployment at the same time. Way nicer.

This looks interesting. Sort of like a `docker run`:

    $ kubectl run curl-test --image=radial/busyboxplus:curl -i --tty --rm
    root@curl-test $ nslookup nginx.default  # that `.default` is the namespace. neat!
    root@curl-test $ nslookup nginx
    root@curl-test $ env | grep NGINX  # woahhh
    NGINX_PORT_80_TCP=tcp://10.103.133.209:80
    NGINX_SERVICE_HOST=10.103.133.209
    NGINX_PORT=tcp://10.103.133.209:80
    NGINX_SERVICE_PORT=80
    NGINX_SERVICE_PORT_WEB=80
    NGINX_PORT_80_TCP_ADDR=10.103.133.209
    NGINX_PORT_80_TCP_PORT=80   # lol, wat?
    NGINX_PORT_80_TCP_PROTO=tcp

> If the Busybox service was created before the Nginx service the URLs would
> still work, but the environment settings would not be present. It's not the
> best idea to rely on these environment settings since the order of creation
> of services is not guaranteed.

Okay... gonna just try to forget about this, then.

> Tip: Try to leave most communication on port 80 or a common port the service
> expects to be on. Later add a meshing network like Istio. Without further
> code modifications, Istio can step in to ensure all intra-communication is
> secured with mutual TLS and monitored for tracing and metrics.

Ok, I've heard of Istio before. Nice to see its reason for existence. Seems
like a bit of a bummer that mutual TLS isn't baked into k8s, but there's
probably a good reason for that that I'm not thinking of.

### 2022-02-06 22:15 -0800: DNS for Services and Pods

    $ kubectl get services,pods,deployments -l 'k8s-app=kube-dns' --all-namespaces
    NAMESPACE     NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
    kube-system   service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   121m

    NAMESPACE     NAME                           READY   STATUS    RESTARTS       AGE
    kube-system   pod/coredns-78fcd69978-p7chd   1/1     Running   2 (117m ago)   121m

    NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
    kube-system   deployment.apps/coredns   1/1     1            1           121m

Ok, CoreDNS. That checks out. We're done here.

### 2022-02-06 22:17 -0800: closing thoughts

> When you decide on a Kubernetes cluster to use, one of the first things you
> should investigate is what service flavor provides the DNS resolutions.

Ok, good to know that this isn't standardized.

Some useful links here:

- [Create Ingress Routing](https://www.katacoda.com/courses/kubernetes/create-kubernetes-ingress-routes)
- [Networking Introduction](https://www.katacoda.com/courses/kubernetes/networking-introduction)
- [Get Started with Istio and Kubernetes](https://www.katacoda.com/courses/istio/deploy-istio-on-kubernetes)

I'll try to remember to come back to these when I'm done with this sequence of scenarios.

## 2022-02-06 22:21 -0800: Sidecar Containers

Ooh, turns out there *is* a difference between Pods and Containers, and now I know why!

> These next steps provide a simple introduction to the idea of putting two
> containers in a Pod to achieve the sidecar pattern. The primary advantage of
> sidecars is a separation of concerns. Each container is highly cohesive in
> terms of responsibilities, yet they work together to provide a single
> solution.

Interesting

> two containers can share information through a shared file mount.

### 2022-02-06 22:28 -0800: step 1: Communication Between Containers

Talk about IPC and PID namespaces. Neat! This feels like functionality I've
never seen in Docker. I like what I'm seeing.

Sidecar containers often share:

- PersistenceVolume mount: "emptyDir"
- localhost

I don't really know what either of these things means. Hopefully we'll learn soon.

Nothing to actually *do* here, moving on.

### 2022-02-06 22:31 -0800: step 2: Deploy Pod with Two Containers


    $ cat sidecar-example.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: sidecar-pod
      labels:
        app: sidecar-pod
    spec:
      volumes:
      - name: shared-data
        emptyDir: {}

      containers:

      - name: first
        image: nginx
        volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html

      - name: second
        image: debian
        volumeMounts:
        - name: shared-data
          mountPath: /pod-data
        command: ["/bin/sh"]
        args:
          - "-c"
          - >
            while true; do
              echo "Hello from the sidecar container. My time is:" >> /pod-data/index.html;
              date >> /pod-data/index.html;
              sleep 1;

    $ kubectl apply -f sidecar-example.yaml

Uhh... `CrashLoopBackOff`?

    $ kubectl get pods -l app=sidecar-pod
    NAME          READY   STATUS             RESTARTS      AGE
    sidecar-pod   1/2     CrashLoopBackOff   3 (26s ago)   86s


Ooh here's what's going on!

    $ kubectl logs -p sidecar-pod -c second
    /bin/sh: 5: Syntax error: end of file unexpected (expecting "done")

Ok, fix bug, and *this* happens?

    $ kubectl apply -f /tmp/sidecar-example.yaml
    The Pod "sidecar-pod" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds`, `spec.tolerations` (only additions to existing tolerations) or `spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
      core.PodSpec{
        Volumes:        {{Name: "shared-data", VolumeSource: {EmptyDir: &{}}}, {Name: "kube-api-access-vnfxk", VolumeSource: {Projected: &{Sources: {{ServiceAccountToken: &{ExpirationSeconds: 3607, Path: "token"}}, {ConfigMap: &{LocalObjectReference: {Name: "kube-root-ca.crt"}, Items: {{Key: "ca.crt", Path: "ca.crt"}}}}, {DownwardAPI: &{Items: {{Path: "namespace", FieldRef: &{APIVersion: "v1", FieldPath: "metadata.namespace"}}}}}}, DefaultMode: &420}}}},
        InitContainers: nil,
        Containers: []core.Container{
            {Name: "first", Image: "nginx", VolumeMounts: {{Name: "shared-data", MountPath: "/usr/share/nginx/html"}, {Name: "kube-api-access-vnfxk", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, TerminationMessagePath: "/dev/termination-log", ...},
            {
                Name:    "second",
                Image:   "debian",
                Command: {"/bin/sh"},
                Args: []string{
                    "-c",
                    (
                        """
                        ... // 2 identical lines
                          date >> /pod-data/index.html;
                          sleep 1;
    - 					done
                        """
                    ),
                },
                WorkingDir: "",
                Ports:      nil,
                ... // 16 identical fields
            },
        },
        EphemeralContainers: nil,
        RestartPolicy:       "Always",
        ... // 25 identical fields
      }

Ok, well since we don't care about uptime, I'm just going to tear down the old pod. How do I do that again?

    $ kubectl delete pod sidecar-pod
    pod "sidecar-pod" deleted

Yay that worked!

    $ kubectl get pods -l app=sidecar-pod
    NAME          READY   STATUS    RESTARTS   AGE
    sidecar-pod   2/2     Running   0          4s

Expose pod via a service:

    $ kubectl expose pod sidecar-pod --type=NodePort --port=80
    service/sidecar-pod exposed
    $ kubectl get services
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    hello         NodePort    10.99.58.155     <none>        8080:31001/TCP   82m
    kubernetes    ClusterIP   10.96.0.1        <none>        443/TCP          159m
    nginx         NodePort    10.103.133.209   <none>        80:30019/TCP     47m
    sidecar-pod   NodePort    10.110.121.82    <none>        80:32444/TCP     9s

    $ kubectl patch service sidecar-pod --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":31111}]'
    service/sidecar-pod patched

Ok, next!


### 2022-02-06 22:55 -0800: part 4: Query Nginx with its Sidecar

This is not super helpful:

    $ kubectl get services
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    hello         NodePort    10.99.58.155     <none>        8080:31001/TCP   83m
    kubernetes    ClusterIP   10.96.0.1        <none>        443/TCP          160m
    nginx         NodePort    10.103.133.209   <none>        80:30019/TCP     48m
    sidecar-pod   NodePort    10.110.121.82    <none>        80:31111/TCP     66s

*This* is useful, but isn't going to help me when I move away from minikube =(

    $ minikube service list
    |----------------------|---------------------------|--------------|---------------------------|
    |      NAMESPACE       |           NAME            | TARGET PORT  |            URL            |
    |----------------------|---------------------------|--------------|---------------------------|
    | default              | hello                     |         8080 | http://192.168.49.2:31001 |
    | default              | kubernetes                | No node port |
    | default              | nginx                     | web/80       | http://192.168.49.2:30019 |
    | default              | sidecar-pod               |           80 | http://192.168.49.2:31111 |
    | kube-system          | kube-dns                  | No node port |
    | kube-system          | metrics-server            | No node port |
    | kubernetes-dashboard | dashboard-metrics-scraper | No node port |
    | kubernetes-dashboard | kubernetes-dashboard      |           80 | http://192.168.49.2:30000 |
    |----------------------|---------------------------|--------------|---------------------------|
    $ curl http://192.168.49.2:31111/


## 2022-02-06 22:59 -0800: Jobs

> A Job creates one or more Pods and will continue to retry execution of the Pods
> until a specified number of them successfully terminate.

oopsies:

> how Job can serially or in parallel,

oooh woah?

> how Jobs can process a work queue.

### 2022-02-07 08:34 -0800: part 1

just some setup again

### 2022-02-07 08:33 -0800: part 2: Run Single Job

Some notes about deprecation. Urg, reading a tutorial about an outdated book is kind of *weird*.

I guess `kubectl run` is simpler than it used to be? "It will now only create pods". Ok.

Sidenote: I've still got a lot of pods running from the past tutorials. Let's
see if I know enough to clean that up now.

    $ k get pods
    NAME                                READY   STATUS    RESTARTS   AGE
    hello-b644646c4-2f52k               1/1     Running   0          9h
    hello-b644646c4-444k8               1/1     Running   0          9h
    hello-b644646c4-j4qqw               1/1     Running   0          9h
    nginx-deployment-7589664b85-ct7mk   1/1     Running   0          10h
    nginx-deployment-7589664b85-jztwv   1/1     Running   0          10h
    sidecar-pod                         2/2     Running   0          9h

    $ kubectl delete -f echoserver.yaml
    $ kubectl delete -f sidecar-example.yaml
    $ k delete -f https://raw.githubusercontent.com/javajon/kubernetes-fundamentals/master/nginx/nginx.yaml
    # lol, cute
    $ k get pods
    No resources found in default namespace.

eep, another typo:

> Once complete you will see no Jobs where started:

typo: where -> were

Here's a big scary command:

    $ kubectl run -i oneshot \
      --image=gcr.io/kuar-demo/kuard-amd64:blue \
      --restart=OnFailure \
      -- \
         /kuard \
         --keygen-enable \
         --keygen-exit-on-complete \
         --keygen-num-to-gen 5
    If you don't see a command prompt, try pressing enter.
    2022/02/07 16:45:59 (ID 0 1/5) Item done: SHA256:4QnqR5t9XB4uE2DtnpcsKqGiidbgK1uLOS1Vj2wFFIQ
    2022/02/07 16:46:00 (ID 0 2/5) Item done: SHA256:ikgaKU4+UFFeMk+2OP8cXxjWHHOCky3qwZzAiz3nkTw
    2022/02/07 16:46:02 (ID 0 3/5) Item done: SHA256:VDNs8AqUHstFzeL2d7/z/qCRvqmNsxeAWx/7Gm6/Qa0
    2022/02/07 16:46:04 (ID 0 4/5) Item done: SHA256:c2rnVmX673TwoB4zCRG5fED+GT4lrrM2iOVVno5ZyKE
    2022/02/07 16:46:06 (ID 0 5/5) Item done: SHA256:ZS2wvo+4WaM0xRVZFCsz2T1j2YvtuzAWCUWn53sI4Gg
    2022/02/07 16:46:06 (ID 0) Workload exiting

    $ k get jobs
    No resources found in default namespace.
    $ k get pods
    NAME      READY   STATUS      RESTARTS   AGE
    oneshot   0/1     Completed   0          38s

    $ minikube dashboard
    # ooh, you can find logs in here. neat!

cleanup

    $ kubectl delete pod oneshot
    pod "oneshot" deleted

### 2022-02-07 08:49 -0800: Job from Resource

YES YAML

    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: oneshot
      labels:
        chapter: jobs
    spec:
      template:
        metadata:
          labels:
            chapter: jobs
        spec:
          containers:
          - name: kuard
            image: gcr.io/kuar-demo/kuard-amd64:1
            imagePullPolicy: Always
            args:
            - "--keygen-enable"
            - "--keygen-exit-on-complete"
            - "--keygen-num-to-gen=10"
          restartPolicy: OnFailure

    # lol, hopefully that yaml file hasn't changed since i curl-ed it...
    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml
    job.batch/oneshot created


    $ k get jobs
    NAME      COMPLETIONS   DURATION   AGE
    oneshot   1/1           24s        44s
    $ k describe jobs oneshot
    ...
      Normal  SuccessfulCreate  49s   job-controller  Created pod: oneshot--1-xj56g
    ...

That `oneshot--1-xj56g` is a "job id".

    $ kubectl logs oneshot--1-xj56g
    ...
    $ k delete jobs oneshot

### 2022-02-07 08:54 -0800: Job Failure

    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-2-job-oneshot-failure1.yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: oneshot
      labels:
        chapter: jobs
    spec:
      template:
        metadata:
          labels:
            chapter: jobs
        spec:
          containers:
          - name: kuard
            image: gcr.io/kuar-demo/kuard-amd64:1
            imagePullPolicy: Always
            args:
            - "--keygen-enable"
            - "--keygen-exit-on-complete"
            - "--keygen-exit-code=1"
            - "--keygen-num-to-gen=3"
          restartPolicy: OnFailure
    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-2-job-oneshot-failure1.yaml
    job.batch/oneshot created

    $ k get pod
    NAME               READY   STATUS    RESTARTS     AGE
    oneshot--1-wj7t9   1/1     Running   1 (4s ago)   12s

Eventually...

    $ k get pod
    NAME               READY   STATUS             RESTARTS      AGE
    oneshot--1-wj7t9   0/1     CrashLoopBackOff   3 (27s ago)   87s

How many restarts? Forever?

> Kubernetes will continue to repeat the lifecycle of the Job. Alternatively,
> the policy could have been set to Never to prevent the restarts.

Cleanup:

    $ k delete jobs oneshot
    job.batch "oneshot" deleted

Would have been nice to see it actually succeed =( Oh well.

### 2022-02-07 08:58 -0800: Parallelism

> Instead of each job creating a series of keys serially,3 have multiple jobs work on smaller units of work.

More typos =( "3 have" -> "have 3"

    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml
    job.batch/oneshot created
    $ echo "Duration: $(expr $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.startTime))) seconds"
    Duration: 21 seconds

Ok, let's try that again with some parallelism!

    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-3-job-parallel.yaml | sed '/num-to-gen=/s/=.*/=1"/' > job-parallel.yaml
    $ cat job-parallel.yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: parallel
      labels:
        chapter: jobs
    spec:
      parallelism: 5
      completions: 10
      template:
        metadata:
          labels:
            chapter: jobs
        spec:
          containers:
          - name: kuard
            image: gcr.io/kuar-demo/kuard-amd64:1
            imagePullPolicy: Always
            args:
            - "--keygen-enable"
            - "--keygen-exit-on-complete"
            - "--keygen-num-to-gen=1"
          restartPolicy: OnFailure

Note the `parallelism: 5` and `completions: 10`.

Let's do it:

    $ k apply -f job-parallel.yaml
    job.batch/parallel created
    $ echo "Duration: $(expr $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.startTime))) seconds"
    Duration: 13 seconds

Cleanup:

    $ k delete jobs --selector chapter=jobs
    job.batch "oneshot" deleted
    job.batch "parallel" deleted

> However, to fully take advantage of parallelism some asynchronous message
> queuing should also be woven into the mix. In the next step, you will
> leverage a queue.

Bring it on!

### 2022-02-07 09:10 -0800: Work Queue

Uh oh, need another yaml file.

    $ cat queue.yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      labels:
        app: work-queue
        component: queue
        chapter: jobs
      name: queue
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: work-queue
      template:
        metadata:
          labels:
            app: work-queue
            component: queue
            chapter: jobs
        spec:
          containers:
          - name: queue
            image: "gcr.io/kuar-demo/kuard-amd64:1"
            imagePullPolicy: Always

    $ kubectl apply -f queue.yaml

    $ k get pods
    NAME          READY   STATUS    RESTARTS   AGE
    queue-spn9c   1/1     Running   0          85s

Interesting:

    $ k port-forward queue-spn9c 8080:8080
    Forwarding from 127.0.0.1:8080 -> 8080
    Forwarding from [::1]:8080 -> 8080

Put a bunch of stuff in the queue:

    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-6-load-queue.sh
    # Create a work queue called 'keygen'
    curl -X PUT localhost:8080/memq/server/queues/keygen

    # Create 100 work items and load up the queue.
    for i in work-item-{0..99}; do
      curl -X POST localhost:8080/memq/server/queues/keygen/enqueue \
        -d "$i"
    done
    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-6-load-queue.sh | bash
    ...
    $ curl -s localhost:8080/memq/server/stats | jq
    {
      "kind": "stats",
      "queues": [
        {
          "name": "keygen",
          "depth": 100,
          "enqueued": 100,
          "dequeued": 0,
          "drained": 0
        }
      ]
    }

Now expose the queue? I guess there's a webui?

    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-5-service-queue.yaml | sed ... # add in spec.type=NodePort

Here's how to create some consumers:

    $ curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-7-job-consumers.yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      labels:
        app: message-queue
        component: consumer
        chapter: jobs
      name: consumers
    spec:
      parallelism: 5
      template:
        metadata:
          labels:
            app: message-queue
            component: consumer
            chapter: jobs
        spec:
          containers:
          - name: worker
            image: "gcr.io/kuar-demo/kuard-amd64:1"
            imagePullPolicy: Always
            args:
            - "--keygen-enable"
            - "--keygen-exit-on-complete"
            - "--keygen-memq-server=http://queue:8080/memq/server"
            - "--keygen-memq-queue=keygen"
          restartPolicy: OnFailure

    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-7-job-consumers.yaml
    job.batch/consumers created

Cleanup:

    $ kubectl delete rs,svc,job --selector chapter=jobs

## 2022-02-07 23:01 -0800: ConfigMaps and Secrets

I'm excited! Let's do this.

### 2022-02-07 23:37 -0800: step 2: Create

    $ kubectl create configmap mountains --from-literal=aKey=aValue --from-literal=14ers=www.14ers.com
    configmap/mountains created
    $ k get configmap mountains
    NAME        DATA   AGE
    mountains   2      4s

Urg I do NOT CARE about this. YAML please.

    $ cat ucs-org.yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: ucs-info
      namespace: default
    data:
      property.1: hello
      property.2: world
      ucs-org: |-
        description="Our scientists and engineers develop and implement innovative, practical solutions to some of our planet's most pressing problems"
        formation=1969
        headquarters="Cambridge, Massachusetts, US"
        membership="over 200,000"
        director="Kathleen Rest"
        president="Kenneth Kimmell"
        founder="Kurt Gottfried"
        concerns="Global warming and developing sustainable ways to feed, power, and transport ourselves, to fighting misinformation, advancing racial equity, and reducing the threat of nuclear war."
        website="ucsusa.org"

Ok, cool! Nicely nested =) Let's see how to access these stuff from an application.


### 2022-02-07 23:42 -0800: CLI

    $ cat consume-via-cli.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: consume-via-cli
    spec:
      containers:
        - name: consuming-container
          image: k8s.gcr.io/busybox
          command: [ "/bin/sh", "-c", "echo $(PROPERTY_ONE_KEY); echo $(UCS_INFO); env" ]
          env:
            - name: PROPERTY_ONE_KEY
              valueFrom:
                configMapKeyRef:
                  name: ucs-info
                  key: property.1
            - name: UCS_INFO
              valueFrom:
                configMapKeyRef:
                  name: ucs-info
                  key: ucs-org
      restartPolicy: Never
    $ k logs consume-via-cli

### 2022-02-07 23:48 -0800: step 4: via env

    $ cat consume-via-env.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: consume-via-env
    spec:
      containers:
        - name: consuming-container
          image: k8s.gcr.io/busybox
          command: [ "/bin/sh", "-c", "env" ]
          envFrom:
          - configMapRef:
              name: ucs-info
      restartPolicy: Never

    $ kubectl create -f consume-via-env.yaml
    $ kubectl logs consume-via-env


### 2022-02-07 23:52 -0800: step 5: volume mounts

    $ cat consume-via-vol.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: consume-via-vol
    spec:
      containers:
        - name: consuming-container
          image: k8s.gcr.io/busybox
          command: [ "/bin/sh","-c","cat /etc/config/keys" ]
          volumeMounts:
          - name: config-volume
            mountPath: /etc/config
      volumes:
        - name: config-volume
          configMap:
            name: ucs-info
            items:
            - key: ucs-org
              path: keys
      restartPolicy: Never


### 2022-02-07 23:58 -0800: step 6: create secret

    $ cat secret.yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: db-creds
    type: Opaque
    data:
      username: dXNlcgo=
      password: TXlEYlBhc3N3MHJkCg==

    $ k get secret db-creds

### 2022-02-08 00:03 -0800: step 7: read secret

    $ cat kuard.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: kuard
      labels:
        app: kuard
    spec:
      containers:
      - image: gcr.io/kuar-demo/kuard-amd64:1
        name: kuard
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-creds
              key: username
        - name: SECRET_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-creds
              key: password

    $ cat kuard-service.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: kuard
      labels:
        app: kuard
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: http
        nodePort: 31001
        protocol: TCP
      selector:
        app: kuard

Oho! Here's how to get those yaml files with secrets in them versioned controlled: https://github.com/bitnami-labs/sealed-secrets


# 2022-03-05 09:00 -0800: trying it out myself

What's the yaml equivalent of this command?

    $ k expose deployment hello --type=NodePort

Oooh, docs: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#expose

Yes!

    $ k get service hello -o yaml

https://kubernetes.io/docs/concepts/services-networking/service/ says:

> Port definitions in Pods have names, and you can reference these names in the
> targetPort attribute of a Service. This works even if there is a mixture of
> Pods in the Service using a single configured name, with the same network
> protocol available via different port numbers. This offers a lot of
> flexibility for deploying and evolving your Services. For example, you can
> change the port numbers that Pods expose in the next version of your backend
> software, without breaking clients.

OH SNAP THIS ACTUALLY DOES JUST WORK:

    http://clark:31482/

This is a neat trick for poking under the hook a little bit:

    k3s crictl ps
    k3s crictl images


# 2022-03-06 00:30 -0800: trying to set up an ingress

Well this is interesting! Merely enabling k3s seems to have gotten in the way of my existing docker container listening on port 80 and 443.
It looks like k3s set up some iptables rules that are getting in the way.

Let's disable port forwarding before we go any further. I don't know what I'm doing, and I don't want the outside world to snoop in on it: http://192.168.1.1/Advanced_VirtualServer_Content.asp. I've removed 443 and 80 from the list.

URG WTF IS THIS: from "Install Traefik Dashboard" on https://pgillich.medium.com/setup-lightweight-kubernetes-with-k3s-6a1c57d62217#08a8

> Traefik Dashboard is not enabled by default. It can be enabled in Helm file of Traefik.
> ... sudo nano /var/lib/rancher/k3s/server/manifests/traefik.yaml ...
> Note: after reboot the Helm file and the ingress config are recovered to the original content.

https://forums.rancher.com/t/k3s-traefik-dashboard-activation/17142/15 seems to be about this, but doesn't have a straightforward answer. I don't see a traefik pod on *my* setup.

Ok so I just tried those ridiculous medium instructions, and I don't see a `traefik-dashboard` showing up on *my* cluster. After a while, still getting this:

    $ k get endpoints traefik-dashboard -n kube-system
    Error from server (NotFound): endpoints "traefik-dashboard" not found

Oooh:

    $ k get pods --all-namespaces
    kube-system   traefik-74dd4975f9-v7jtd                 1/1     Running     0          26h

    $ k describe pod traefik-74dd4975f9-v7jtd -n kube-system | grep dashboard
      --api.dashboard=true


# 2022-03-06 11:18 -0800: let's try a LoadBalancer instead

Ok, so I spent a *little* while trying to get a traefik ingress working and gave up.

Trying out a Service iwth `type: LoadBalancer` now. Let's see how it goes!



# 2022-03-06 23:40 -0800: back to traefik

got it working! now muddling through https + lets encrypt

From https://traefik.io/blog/https-on-kubernetes-using-traefik-proxy/

    $ cat openssl.conf
    [ req ]
    default_bits       = 2048
    distinguished_name = req_distinguished_name
    req_extensions     = req_ext
    prompt = no
    [ req_distinguished_name  ]
    C = US
    ST = VA
    CN  = clark.snowdon.jflei.com
    [ req_ext ]
    subjectAltName = @alt_names
    [ alt_names ]
    DNS.1 = clark.snowdon.jflei.com
    $ openssl req  -nodes -new -x509  -keyout server.key -out server.crt -config openssl.conf -days 365
    ...
    $ kubectl create secret generic testsecret-tls --from-file=tls.crt=./server.crt --from-file=tls.key=./server.key


Ok now for reals:

    $ wget https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.yaml
    $ k apply -f cert-manager.yaml
    $ wget https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cmctl-linux-amd64.tar.gz
    $ rm LICENSES cmctl-linux-amd64.tar.gz

edit + add "ssl.enforced: "true"

    /var/lib/rancher/k3s/server/manifests/traefik.yaml


# 2022-03-07 15:03 -0800: afternoon chat w/ guidry

THE NOUNS!

    $ k api-resources


note that some of these are namespaced, but not all

`-o wide`, `-o yaml`

pod HostPath is how you will thread NFS stuff through

PersistentVolumeClaim is how something like longhorn would work

http -> https redirection:

    ---
    apiVersion: traefik.containo.us/v1alpha1
    kind: Middleware
    metadata:
      namespace: skim
      name: skim-https-redirect
    spec:
      redirectScheme:
        scheme: https
        permanent: true
    ---
    apiVersion: traefik.containo.us/v1alpha1
    kind: IngressRoute
    metadata:
      namespace: skim
      name: skim-http
    spec:
      entryPoints:
        - web
      routes:
        - match: Host(`skim.home.theguidrys.us`)
          kind: Rule
          services:
            - name: skim
              port: 5000
          middlewares:
          - name: skim-https-redirect
    ---
    apiVersion: traefik.containo.us/v1alpha1
    kind: Middleware
    metadata:
      namespace: skim
      name: skim-basic-auth
    spec:
      basicAuth:
        secret: skim-htpasswd
    ---
    apiVersion: traefik.containo.us/v1alpha1
    kind: IngressRoute
    metadata:
      namespace: skim
      name: skim
    spec:
      entryPoints:
        - websecure
      routes:
      - match: Host(`skim.home.theguidrys.us`)
        kind: Rule
        services:
        - name: skim
          port: 5000
        middlewares:
        - name: skim-basic-auth
          namespace: skim
      tls:
        secretName: skim-tls-cert
    ---

# 2022-04-14 01:32 -0700: figuring out vpn on k8s

I tried following https://docs.k8s-at-home.com/guides/pod-gateway/, but it just didn't work out nicely:

1. gateway is wrong ip? comes from here: https://github.com/k8s-at-home/gateway-admision-controller/blob/81a62e4c534febfd59ec4249b1999588ad7cd17f/internal/mutation/gatewayPodMutator.go#L166
    i dug, and this is actually coming straight from /etc/resolv.conf in the pod: https://github.com/k8s-at-home/gateway-admision-controller/blob/81a62e4c534febfd59ec4249b1999588ad7cd17f/internal/resolv/resolv.go#L26. i don't know what the bug is here. maybe this works for people not using k3s? or using a different version/configuration of k3s? this would probably be the place to file an issue: https://github.com/k8s-at-home/gateway-admision-controller/issues
2. weirdness in generated /etc/dhclient.conf
   - `link-timeout` is not a recognized command? https://github.com/k8s-at-home/pod-gateway/blob/ac6ade8ad89db937ee1bd7d99217d108ce1f20f0/bin/client_init.sh#L58
   - commented out `domain-name-servers` doesn't work: https://github.com/k8s-at-home/pod-gateway/blob/ac6ade8ad89db937ee1bd7d99217d108ce1f20f0/bin/client_init.sh#L69
3. `dhclient -v -cf /etc/dhclient.conf vxlan0` fails with error: `mv: cannot move '/etc/resolv.conf.dhclient-new.NEh8nxFriC' to '/etc/resolv.conf': Resource busy`
    looks like dhclient *itself* is suffering from https://stackoverflow.com/a/60576223/1739415

Oooh https://docs.k8s-at-home.com/guides/pod-gateway/#routed-pod-fails-to-init says to try `NOT_ROUTED_TO_GATEWAY_CIDRS`

AND OMG IT IS WORKING NOW. YESSSSSSSSSSSSS

# 2022-04-14 15:04 -0700: now trying to get wireguard running

This is not going well.


    apiVersion: v1
    kind: Pod
    metadata:
      name: jfly-test
    spec:
      containers:
      - image: ghcr.io/k8s-at-home/wireguard:v1.0.20210914
        imagePullPolicy: IfNotPresent
        name: wireguard
        command: ["tail", "-f", "/dev/null"]
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
            - SYS_MODULE
        volumeMounts:
        - mountPath: /etc/wireguard/wg0.conf
          name: vpnconfig
          subPath: vpnConfigfile
        - name: vpnconfig
          mountPath: "/etc/foo"
          subPath: vpnConfigfile
      volumes:
      - name: vpnconfig
        secret:
          #<<< defaultMode: 420
          defaultMode: 0777
          items:
          - key: vpnConfigfile
            path: vpnConfigfile
          secretName: wireguard

This is hilarious: https://github.com/k8s-at-home/container-images/blob/51cd7ce8c72ae9cae30edb0ba92da6486034ef5a/apps/wireguard/build/sudoers_kah#L14. Haxors:

    $ k exec -it jfly-test -- bash
    $ sudo /usr/bin/find /etc/wireguard -type f -printf "%f\n" -exec bash \;

And now I'm root, lol

Trying to get this command to pass:

    $ k exec -it jfly-test -- bash
    kah@jfly-test:/tmp$ sudo /usr/bin/wg-quick up wg0
    [#] ip link add wg0 type wireguard
    [#] wg setconf wg0 /dev/fd/63
    [#] ip -4 address add 10.67.182.177/32 dev wg0
    [#] ip -6 address add fc00:bbbb:bbbb:bb01::4:b6b0/128 dev wg0
    [#] ip link set mtu 1370 up dev wg0
    [#] resolvconf -a wg0 -m 0 -x
    [#] wg set wg0 fwmark 51820
    [#] ip -6 route add ::/0 dev wg0 table 51820
    [#] ip -6 rule add not fwmark 51820 table 51820
    [#] ip -6 rule add table main suppress_prefixlength 0
    [#] ip6tables-restore -n
    ip6tables-restore v1.8.4 (legacy): ip6tables-restore: unable to initialize table 'raw'

    Error occurred at line: 1
    Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
    [#] resolvconf -d wg0 -f
    [#] ip -6 rule delete table 51820
    [#] ip -6 rule delete table main suppress_prefixlength 0
    [#] ip link delete dev wg0
    kah@jfly-test:/tmp$ echo $?
    2

TODO: play with backends?

    $ k run -it jfly-test --image=ghcr.io/k8s-at-home/wireguard:v1.0.20210914 --command=true -- bash
    $ k exec -it jfly-test -- bash
    IPTABLES_BACKEND=nft /entrypoint.sh   # this hung, but seemed to do what i want?


# 2022-04-16 10:46 -0700: port forwarding now

uhhhh, nice grep? that's not gonna do exactly what you want... https://github.com/k8s-at-home/pod-gateway/blob/ac6ade8ad89db937ee1bd7d99217d108ce1f20f0/bin/client_init.sh#L40
